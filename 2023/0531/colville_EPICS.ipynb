{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Basic Bayesian Optimization\n",
    "In this tutorial we demonstrate the use of Xopt to preform Bayesian Optimization on a\n",
    " simple test problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define the test problem\n",
    "Here we define a simple optimization problem, where we attempt to minimize the sin\n",
    "function in the domian [0,2*pi]. Note that the function used to evaluate the\n",
    "objective function takes a dictionary as input and returns a dictionary as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xopt.vocs import VOCS\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# difine dimension of function\n",
    "N = 4 \n",
    "\n",
    "# define variables and function objectives\n",
    "vocs = VOCS(\n",
    "    variables={ \"x\" + str(i) : [0,2] for i in range (N)},\n",
    "    objectives={\"f\": \"MINIMIZE\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from epics import caget , caput\n",
    "import time\n",
    "\n",
    "\n",
    "def Colville_EPICS_fun(inputs):\n",
    "    caput ('TEST:X0', inputs[\"x0\"])\n",
    "    caput ('TEST:X1', inputs[\"x1\"])\n",
    "    caput ('TEST:X2', inputs[\"x2\"])\n",
    "    caput ('TEST:X3', inputs[\"x3\"])\n",
    "    time.sleep (0.1)\n",
    "    return{\"f\": caget('TEST:Y')}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Xopt objects\n",
    "Create the evaluator to evaluate our test function and create a generator that uses\n",
    "the Upper Confidence Bound acqusition function to perform Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xopt.evaluator import Evaluator\n",
    "from xopt.generators.bayesian import UpperConfidenceBoundGenerator\n",
    "from xopt import Xopt\n",
    "\n",
    "evaluator = Evaluator(function=Colville_EPICS_fun)\n",
    "generator = UpperConfidenceBoundGenerator(vocs)\n",
    "X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate and evaluate initial points\n",
    "To begin optimization, we must generate some random initial data points. The first call\n",
    "to `X.step()` will generate and evaluate a number of randomly points specified by the\n",
    " generator. Note that if we add data to xopt before calling `X.step()` by assigning\n",
    " the data to `X.data`, calls to `X.step()` will ignore the random generation and\n",
    " proceed to generating points via Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print initial number of points to be generated\n",
    "print(X.generator.options.n_initial)\n",
    "\n",
    "# call X.step() to generate + evaluate initial points\n",
    "X.step()\n",
    "\n",
    "# inspect the gathered data\n",
    "X.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Do bayesian optimization steps\n",
    "To perform optimization we simply call `X.step()` in a loop. This allows us to do\n",
    "intermediate tasks in between optimization steps, such as examining the model and\n",
    "acquisition function at each step (as we demonstrate here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1,ax1 = plt.subplots(1,1,figsize=[15,10],facecolor = 'lightgray')\n",
    "\n",
    "x = {}\n",
    "y = {}\n",
    "ymin = 1e+10\n",
    "\n",
    "for i in range(10):\n",
    "    # get the Gaussian process model from the generator\n",
    "    model = X.generator.train_model()\n",
    "\n",
    "    # get acquisition function from generator\n",
    "    acq = X.generator.get_acquisition(model)\n",
    "\n",
    "    # calculate model posterior and acquisition function at each test point\n",
    "    # NOTE: need to add a dimension to the input tensor for evaluating the\n",
    "    # posterior and another for the acquisition function, see\n",
    "    # https://botorch.org/docs/batching for details\n",
    "    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n",
    "    # skipping calculations for backpropagation\n",
    "\n",
    "    X.step()\n",
    "\n",
    "    x[i] = i+1\n",
    "    y[i] = X.data[\"f\"][i+1]\n",
    "\n",
    "    if  y[i] < ymin:\n",
    "        ymin =  y[i]\n",
    "\n",
    "    print('debug',i, y[i], ymin)\n",
    "\n",
    "    ax1.plot(x[i] ,y[i], \"C0o\" , markersize = 10)\n",
    "    ax1.plot(x[i] ,ymin, \"C1o\" , markersize = 10)\n",
    "\n",
    "ax1.grid(which = \"major\" , color = \"black\" , linestyle = \"-\")\n",
    "ax1.set_xlabel(\"loop count\")\n",
    "ax1.set_ylabel(\"Optimization value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2,ax2 = plt.subplots(N,N,figsize=[18,15],facecolor = 'lightgray')\n",
    "\n",
    "for i in range (N):\n",
    "    for j in range (N-i):\n",
    "\n",
    "        ax2[j,N-1-i].plot(X.data[\"x\" + str(j)], X.data[\"x\" + str(N-1-i)] , marker='o' , label = \"[ \" + str(j) + \", \" + str(N-1-i) + \" ] projection graph\" )\n",
    "        ax2[j,N-1-i].legend()\n",
    "        ax2[j,N-1-i].grid()\n",
    "        ax2[j,N-1-i].set_xlabel(\"x\" + str(N-1-i))\n",
    "        ax2[j,N-1-i].set_ylabel(\"x\" + str(j))\n",
    "\n",
    "# # do the optimization step\n",
    "# X.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Getting the optimization result\n",
    "To get the ideal point (without evaluating the point) we ask the generator to\n",
    "generate a new point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.generator.get_optimum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Customizing optimization\n",
    "Each generator has a set of options that can be modified to effect optimization behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.generator.options.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: add a Gamma(1.0,10.0) prior to the noise hyperparameter to reduce model noise\n",
    "# (good for optimizing noise-free simulations)\n",
    "X.generator.options.model.use_low_noise_prior = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0de745dc68a243c7b25579518615d6729841007b77a800fe9a7ec52c3c26a69b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
